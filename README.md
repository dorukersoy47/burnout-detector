# Lighthouse.ai â€” *â€œAI agents that investigate why your team's productivity dropsâ€”not just that it did.â€*  
![Lighthouse.ai Logo Placeholder](docs/assets/logo-placeholder.png)

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](#) [![License](https://img.shields.io/badge/license-MIT-blue.svg)](#license) [![AgentVerse Hackathon](https://img.shields.io/badge/AgentVerse-2025-orange.svg)](#hackathon-context)

---

## ðŸš¨ Problem

Engineering leads lose weeks chasing the root cause of productivity dips. Dashboards show *what* changedâ€”â€œPR review time up 442%â€â€”but they never answer *why*. Meanwhile, teams burn out, delivery commitments slip, and institutional knowledge fades.

> **Typical spiral:** manual log-dives â†’ meetings with every stakeholder â†’ trial-and-error fixes â†’ repeat next quarter. Lighthouse.ai stops that spiral.

---

## ðŸ› ï¸ Solution Overview

Lighthouse.ai is an autonomous productivity intelligence system. It detects anomalies, runs a ReAct-style investigation to prove causation, plans targeted interventions, routes approvals, executes changes, and learns from real outcomes.

```
[ Anomaly Detector ] â†’ [ Root Cause Investigator ðŸ¤– ] â†’ [ Intervention Planner ] 
              â†“                                   â†“
    Evidence sources (GitHub, Slack, Calendar, Jira via tools/)
              â†“                                   â†“
[ Supervisor & Approvals ] â†’ [ Outcome Monitor ] â†’ [ RLHF Learning Loop ]
```

---

## âœ¨ Key Features

- ðŸ§  **Autonomous Root-Cause Reasoning** â€“ ReAct loop with chain-of-thought transparency  
- ðŸ” **Causal Attribution** â€“ Proves *why* metrics changed, not just that they did  
- ðŸ“š **RLHF Memory** â€“ Learns from every intervention to boost success rates  
- ðŸ¤ **Human-in-the-Loop Safety** â€“ Approval routing & risk guardrails  
- ðŸ“ˆ **Outcome Monitoring** â€“ Closes the loop with measurable impact  
- âš™ï¸ **API-Ready Architecture** â€“ Mock tools today, ready for live integrations tomorrow

---

## ðŸ”„ How It Works

1. **Daily Anomaly Sweep** â€“ `AnomalyDetector` checks productivity stats (Z-scores, baselines).  
2. **Root Cause Investigation** â€“ LLM-powered agent plans hypotheses, calls tools dynamically, and narrates every thought.  
3. **Intervention Planning** â€“ Ranks options using historical cases (RLHF DB).  
4. **Supervisor Orchestration** â€“ Validates evidence, routes approvals, executes recommended changes.  
5. **Outcome Monitoring** â€“ Tracks improvements, detects side effects, and updates success rates.

---

## ðŸ¤– What Makes It Agentic

- Chooses its own investigation path; no static playbook  
- Uses tool outputs to revise hypotheses mid-flight  
- Explains each decision in a readable chain-of-thought log  
- Learns from feedback via the RLHF success database  
- Balances autonomy with human approvals based on risk

---

## âš¡ Quick Start

```bash
# Clone
git clone https://github.com/your-org/lighthouse-ai.git
cd lighthouse-ai

# (Optional) create env
python3 -m venv .venv && source .venv/bin/activate

# Install deps (mock integrations today)
pip install -r requirements.txt
```

If you have an OpenAI API key, add it to `config.py`; otherwise the MVP uses mocked tool responses.

---

## ðŸš€ Usage Examples

### Run the end-to-end demo
```bash
python orchestration/demo_full_workflow.py
```

### Use individual agents
```python
from agents.anomaly_detector import AnomalyDetector
alert = AnomalyDetector().check_for_anomalies()

from agents.root_cause_investigator import RootCauseInvestigator
investigation = RootCauseInvestigator().investigate(alert)

from agents.intervention_planner import InterventionPlanner
plan = InterventionPlanner().plan(investigation.root_cause, {})
```

---

## ðŸ—‚ï¸ Project Structure

```
burnout-detector/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ config.py                    # Global settings & API keys (mock-friendly)
â”œâ”€ agents/                      # Autonomous components
â”‚  â”œâ”€ anomaly_detector.py       # Daily metric anomaly detection
â”‚  â”œâ”€ root_cause_investigator.py# ReAct investigation with verbose reasoning trace
â”‚  â”œâ”€ intervention_planner.py   # RLHF-powered recommendation engine
â”‚  â”œâ”€ supervisor_agent.py       # Validation, approvals, execution
â”‚  â””â”€ outcome_monitor.py        # Post-intervention tracking & RLHF updates
â”œâ”€ orchestration/
â”‚  â””â”€ demo_full_workflow.py     # Orchestrated end-to-end storyline for the MVP
â”œâ”€ tools/
â”‚  â””â”€ GitHub-Tracker/           # Mock data services (ready to swap with live APIs)
â”œâ”€ motion/
â”‚  â””â”€ eye.py                    # Example of sensor-style data source
â”œâ”€ demo_data/
â”‚  â””â”€ GitHub/                   # CSVs feeding mock providers
â””â”€ tests/
   â””â”€ test_systems.py           # System-level tests & guardrails
```

**Interaction Map**

- `orchestration/demo_full_workflow.py` stitches the agents together.  
- Each agent pulls data via mock tool adapters under `tools/`.  
- `OutcomeMonitor` writes learnings back to the RLHF store (in-memory today).  
- Tests ensure regressions are caught when swapping mocks for real APIs.

---

## â–¶ï¸ Demo Instructions

1. Ensure dependencies are installed (see Quick Start).  
2. (Optional) Configure OpenAI credentials in `config.py`.  
3. Run `python orchestration/demo_full_workflow.py`.  
4. Observe console output: anomaly detection, full investigation trace, planning, supervisor decisions, and monitoring summary.

---

## ðŸ—ƒï¸ RLHF Database Schema (MVP)

```text
historical_cases
â”œâ”€ case_id (str)
â”œâ”€ root_cause (str)
â”œâ”€ intervention (str)
â”œâ”€ outcome ("success" | "partial" | "failed")
â”œâ”€ success_metric_improvement (float)
â”œâ”€ team_size (int)
â”œâ”€ days_to_effect (int)
â””â”€ notes (str)

rlhf_feedback
â”œâ”€ intervention (str)
â”œâ”€ outcome (str)
â”œâ”€ success_rate (float)
â””â”€ updated_at (datetime)
```

Currently stored in-memory within `intervention_planner.py`; ready to migrate to Postgres/Vector DB.

---

## ðŸ§  Example Investigation Output

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STEP 1: AGENT REASONING
ðŸ§  AGENT THINKS:
"PR review time jumped 5Ã— and 12 PRs blocked. Hypothesis: senior reviewers stuck in meetings."
...
ACTION & EVIDENCE
ðŸ› ï¸  AGENT DECISION 1: calling `calendar_get_events`
   Parameters:
   { "person": "alice", "days": 14 }
ðŸ¤– TOOL RESULT:
{ "meeting_load_hours": 28, "note": "New 2-hour strategy meeting began Oct 24" }

ðŸ› ï¸  AGENT DECISION 2: calling `github_get_pr_metrics`
...
âœ… Root Cause Identified (Confidence: 0.92)
ðŸ“Œ Root Cause Summary: New daily strategy meeting consuming senior reviewersâ€™ time.
```

---

## ðŸ† Why This Wins

- **First-mover causal reasoning** for developer productivity  
- **Transparent agent**â€”judges see every hypothesis, not just the answer  
- **RLHF loop** compounding value with every success or failure  
- **Built for enterprises**: approvals, audit trail, side-effect monitoring  
- **Pluggable architecture**â€”swap mocks for real APIs when integrating with customers

---

## ðŸ Hackathon Context

- **Event:** AgentVerse Hackathon 2025  
- **Track:** Reimagine the Workplace  
- **Focus:** Autonomous agents boosting team productivity & wellbeing

---

## ðŸ›£ï¸ Roadmap

- ðŸ”Œ Production-grade GitHub, Slack, Jira, and Calendar connectors  
- ðŸ§  Organization-specific RLHF tuning  
- ðŸ”® Predictive anomaly alerts (before metrics crater)  
- ðŸ“Š Web dashboard + Slack concierge bot  
- ðŸŒ Multi-team rollups and cross-team insights

---

## ðŸ¤ Contributing

1. Fork the repo and create a feature branch.  
2. Follow the existing module layout (`agents/`, `orchestration/`, `tools/`).  
3. Add/Update tests in `tests/test_systems.py`.  
4. Submit a PR with a helpful summary and screenshots/logs of your agent trace.

Weâ€™re especially looking for contributors who can:  
- Implement real connectors in `tools/` (`slack_client.py`, `calendar_client.py`, etc.).  
- Expand RLHF persistence beyond in-memory structures.  
- Create a web dashboard or Slack interface.

---

## ðŸ‘¥ Team & Contact

- *Team Lighthouse* â€” AgentVerse Hackathon finalists  
- ðŸ“§ Contact: team@lighthouse.ai (placeholder)  
- ðŸŒ Website: https://lighthouse.ai (placeholder)  
- LinkedIn, Twitter, and demo video placeholders to be added

---

## ðŸ“¸ Screenshots & Demo Video (placeholders)

- ![Investigation Trace Screenshot](docs/assets/screenshots/investigation.png)  
- [ðŸŽ¥ Demo Video](https://youtu.be/placeholder)

> *Replace with real media before launch.*

---

## ðŸ“œ License

This project is licensed under the MIT License â€“ see [LICENSE](LICENSE) for details.

---

## ðŸ™ Acknowledgments

- AgentVerse Hackathon organizers and mentors  
- Open-source contributors who inspired our ReAct implementation  
- Mock data sets courtesy of the teamâ€™s internal playbooks

---

> **Ready for takeoff:** Lighthouse.ai is already illuminating the dark corners of productivity loss. Letâ€™s bring causality-driven insight to every engineering org.

# burnout-detector
